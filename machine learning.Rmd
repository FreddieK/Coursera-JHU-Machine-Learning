---
title: "Course Project - Practical Machine Learning"
author: "Freddie Karlbom"
date: "7/29/2017"
output: 
  html_document: 
    theme: spacelab
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)

# Utilise parallell processing
library(doMC)
registerDoMC(cores = 4)

library(caret)
library(randomForest)
library(tidyverse)
library(magrittr)
library(anytime)

set.seed(1337)
```


## Project Outline
Classify how a workout excercise was performed based on sensor data and use the trained model to predict classification of 20 additional rows.

* You should create a report describing how you built your model, 
* how you used cross validation, 
* What you think the expected out of sample error is, and why you made the choices you did. 

- a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).


## Data analysis
When exploring the data, it's clear that for very many features in the set are very sparse and contains very little data. In order to reduce the number of features and the complexity, we can thus look at removing unneccessary features that doesn't contain much data.
```{r data-analysis}
rawDataSet <- read_csv('./data/pml-training.csv')

hasDataRate <- apply(rawDataSet, 2, function(x) { sum(!is.na(x))/length(x) })
plot(hasDataRate)

```

Based on this graph we can set a low threshold for 10% of the rows filled.

Further, when looking at the time axis, the distribution has stayed the same over time so we can remove these columns as well from the model to simplify.
```{r data-analysis2}
classOverTime <- rawDataSet %>% dplyr::select(raw_timestamp_part_1, classe) %>% as.data.frame()
classOverTime$raw_timestamp_part_1 %<>% anytime() %>% round("day") %>% as.POSIXct()
classOverTime %<>% group_by(raw_timestamp_part_1, classe) %>% count()
colnames(classOverTime) <- c('Date', 'Class', 'Frequency')

g <- ggplot(classOverTime, aes(x=Date, y = Frequency, fill=Class))
g + geom_area(position = 'stack')
```


## Clean and prepare data
Besides filtering out all the columns I don't want to have as predictors, I convert relevant classes to factors and split the data into a training set, consisting of 80%, and a testing set with the remaining data.

Since there are a lot of missing values still in the dataset, I use rfImpute to impute values into the training set.
```{r data-preparation}
fullSet <- rawDataSet

# Remove columns that contain very little data to decrease number of predictors
has_data <- function(x) { sum(!is.na(x))/length(x) > 0.1 }
fullSet %<>% select_if(has_data)

# Remove row number and time stamps as they shouldn't matter for classification
fullSet %<>% dplyr::select(-X1, -raw_timestamp_part_1, -raw_timestamp_part_2, -cvtd_timestamp)

fullSet$user_name %<>% factor()
fullSet$new_window %<>% factor()
fullSet$classe %<>% factor()

inTraining <- createDataPartition(fullSet$classe, p=0.8, list=FALSE)
training <- fullSet[inTraining,]
test <- fullSet[-inTraining,]

if(!file.exists("./models/trainingImputed.rds")){
    training.imputed <- rfImpute(classe ~ ., training)
    saveRDS(training.imputed, "./models/trainingImputed.rds")
} else {
    training.imputed <- readRDS("./models/trainingImputed.rds")
}
```


## Building Models
I explored a few different alghorithms on a smaller set of data including QDA and GLM but settled on using Random Forest and GBM as those were the main classification alghorithms covered in the course. As a final one, I decided to include xgbTree as that is considered one of the best currently available for many usecases.

Random Forest and xgbTree worked very well with default settings, but GBM required some tuning as its default number of trees and interaction depths weren't finegrained enough.

```{r building-models}
# Random Forest
if(!file.exists("./models/rfModel.rds")){
    rf.model <- train(classe ~., data=training.imputed, method="rf")
    saveRDS(rf.model, "./models/rfModel.rds")
} else {
    rf.model <- readRDS("./models/rfModel.rds")
}

# GBM
if(!file.exists("./models/gbmModel.rds")){
    fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
    gbmGrid <- expand.grid(interaction.depth = c(1, 3, 5),
                       n.trees = (1:10)*50,
                       shrinkage = 0.1,
                       n.minobsinnode = 10)
    
    gbm.model <- train(classe ~., 
                    data=training.imputed, 
                    method="gbm",
                    distribution="multinomial",
                    tuneGrid = gbmGrid,
                    trControl = fitControl)
    
    saveRDS(gbm.model, "./models/gbmModel.rds")
} else {
    gbm.model <- readRDS("./models/gbmModel.rds")
}

# XGBoost
if(!file.exists("./models/xgbModel.rds")){
    xgb.model <- train(classe ~., data=training.imputed, method="xgbTree")
    saveRDS(xgb.model, "./models/xgbModel.rds")
} else {
    xgb.model <- readRDS("./models/xgbModel.rds")
}
```


## Evaluation
When using the trained models created on the 20% test set I set aside, all models performed well but none is perfect.

```{r evaluation}
predicted.rf.testing <- predict(rf.model, newdata = test)
predicted.gbm.testing <- predict(gbm.model, newdata = test)
predicted.xgb.testing <- predict(xgb.model, newdata = test)
actual.testing <- test$classe

cfm.rf <- confusionMatrix(actual.testing, predicted.rf.testing)
cfm.gbm <- confusionMatrix(actual.testing, predicted.gbm.testing)
cfm.xgb <- confusionMatrix(actual.testing, predicted.xgb.testing)

cfm.output <- data.frame(cfm.rf$overall, cfm.gbm$overall, cfm.xgb$overall)
colnames(cfm.output) <- c('Random Forest', 'GBM', 'XGB')
knitr::kable(t(cfm.output))
```

In order to utilise the strengths of the three models, I built a final ensemble model using the predictions of the previous models as input.

```{r combined-model}
# A final model that combines all previous
combination.df <- data.frame(rf = predicted.rf.testing, 
                             gbm = predicted.gbm.testing, 
                             xgb = predicted.xgb.testing,
                             classe=actual.testing)

if(!file.exists("./models/combinationModel.rds")){
    combination.model <- train(classe ~ ., method="rf", data=combination.df)
    saveRDS(combination.model, "./models/combinationModel.rds")
} else {
    combination.model <- readRDS("./models/combinationModel.rds")
}

predicted.combination.testing <- predict(combination.model, dplyr::select(combination.df, -classe))

cfm.combined <- confusionMatrix(actual.testing, predicted.combination.testing)

cfm.combined.output <- data.frame(cfm.combined$overall)
colnames(cfm.combined.output) <- c('Ensemble Model')
knitr::kable(t(cfm.combined.output))
```

When looking at the models feature importance, it is clearly blending the different models together. This seemed to work well for the submitted quizz test data too, which got 100% accuracy.
```{r feature-blending}
varImp.combination <- varImp(combination.model)
featureImportance <- varImp.combination$importance %>% 
    tibble::rownames_to_column() %>% 
    arrange(desc(Overall))
colnames(featureImportance) <- c('Feature', 'Importance')
knitr::kable(featureImportance)
```

```{r quizz-predictions}
predictionSet <- read_csv('./data/pml-testing.csv')

predicted.rf.final <- predict(rf.model, newdata = predictionSet)
predicted.gbm.final <- predict(gbm.model, newdata = predictionSet)
predicted.xgb.final <- predict(xgb.model, newdata = predictionSet)

combination.df.final <- data.frame(rf = predicted.rf.final, 
                             gbm = predicted.gbm.final, 
                             xgb = predicted.xgb.final)

predicted.combination.final <- predict(combination.model, combination.df.final)
```


## Appendix

### Setup
```{r, ref.label='setup', eval = FALSE, echo = TRUE}
```

### Data Analysis
```{r, ref.label='data-analysis', eval = FALSE, echo = TRUE}
```
```{r, ref.label='data-analysis2', eval = FALSE, echo = TRUE}
```

### Data Preparation
```{r, ref.label='data-preparation', eval = FALSE, echo = TRUE}
```

### Building Models
```{r, ref.label='building-models', eval = FALSE, echo = TRUE}
```

### Evaluation
```{r, ref.label='evaluation', eval = FALSE, echo = TRUE}
```

### Combined Model
```{r, ref.label='combined-model', eval = FALSE, echo = TRUE}
```

### Feature Blending
```{r, ref.label='feature-blending', eval = FALSE, echo = TRUE}
```

### Quizz Predictions
```{r, ref.label='quizz-predictions', eval = FALSE, echo = TRUE}
```